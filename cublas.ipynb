{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-il52DWnFie",
        "outputId": "c0878b60-17f8-48c1-97a5-9474849f2baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "// Helper function for checking CUDA API errors\n",
        "#define CHECK_CUDA(call) do { \\\n",
        "    cudaError_t err = call; \\\n",
        "    if (err != cudaSuccess) { \\\n",
        "        fprintf(stderr, \"CUDA Error at %s:%d - %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "        exit(EXIT_FAILURE); \\\n",
        "    } \\\n",
        "} while (0)\n",
        "\n",
        "// Helper function for checking cuBLAS API errors\n",
        "#define CHECK_CUBLAS(call) do { \\\n",
        "    cublasStatus_t status = call; \\\n",
        "    if (status != CUBLAS_STATUS_SUCCESS) { \\\n",
        "        fprintf(stderr, \"cuBLAS Error at %s:%d - Status %d\\n\", __FILE__, __LINE__, status); \\\n",
        "        /* You might want a function to convert cublasStatus_t to string here */ \\\n",
        "        exit(EXIT_FAILURE); \\\n",
        "    } \\\n",
        "} while (0)\n",
        "\n",
        "\n",
        "int main() {\n",
        "    // --- Problem Definition ---\n",
        "    // We want to compute y = A*x where A is 3x2, x is 2x1, y is 3x1.\n",
        "    // A is stored as a submatrix of a larger 5x2 matrix (column-major).\n",
        "    // x uses non-contiguous elements from a larger vector (size 3, incx=2).\n",
        "\n",
        "    const int M = 3; // Rows of matrix A (and vector y)\n",
        "    const int N = 2; // Columns of matrix A (and vector x)\n",
        "\n",
        "    // --- Storage Definition ---\n",
        "    const int lda = 5;        // Leading dimension of A (rows of the parent matrix)\n",
        "    const int incx = 2;       // Increment for vector x\n",
        "    const int incy = 1;       // Increment for vector y (contiguous)\n",
        "\n",
        "    const int A_parent_rows = lda;\n",
        "    const int A_parent_cols = N;\n",
        "    const size_t A_alloc_size = (size_t)A_parent_rows * A_parent_cols; // Total elements in parent A storage\n",
        "\n",
        "    // We need space for 1 + (N-1)*incx elements for x storage conceptually.\n",
        "    // To keep it simple based on user request (vector length 3, incx 2 uses indices 0 and 2):\n",
        "    const int x_alloc_count = 3;\n",
        "    const size_t X_alloc_size = x_alloc_count; // Total elements in x storage\n",
        "\n",
        "    const size_t Y_alloc_size = M; // Total elements in y storage (contiguous)\n",
        "\n",
        "\n",
        "    // --- Host Data Initialization ---\n",
        "    float *h_A, *h_x, *h_y;\n",
        "    h_A = (float*)malloc(A_alloc_size * sizeof(float));\n",
        "    h_x = (float*)malloc(X_alloc_size * sizeof(float));\n",
        "    h_y = (float*)malloc(Y_alloc_size * sizeof(float));\n",
        "\n",
        "    if (!h_A || !h_x || !h_y) {\n",
        "        fprintf(stderr, \"Failed to allocate host memory\\n\");\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "\n",
        "    printf(\"Initializing host data...\\n\");\n",
        "    // Initialize parent matrix A (Column Major!)\n",
        "    // Conceptual 3x2 A is the top-left part:\n",
        "    // [ 1.0  4.0 ]\n",
        "    // [ 2.0  5.0 ]\n",
        "    // [ 3.0  6.0 ]\n",
        "    // Parent 5x2 matrix storage (Column Major):\n",
        "    // Col 0: [ 1.0, 2.0, 3.0, 7.0, 8.0 ]  <- lda = 5\n",
        "    // Col 1: [ 4.0, 5.0, 6.0, 9.0, 10.0] <- lda = 5\n",
        "    float A_vals[A_alloc_size] = {\n",
        "        1.0f, 2.0f, 3.0f, 7.0f, 8.0f,  // Column 0\n",
        "        4.0f, 5.0f, 6.0f, 9.0f, 10.0f  // Column 1\n",
        "    };\n",
        "    for(size_t i = 0; i < A_alloc_size; ++i) h_A[i] = A_vals[i];\n",
        "\n",
        "    // Initialize vector x storage\n",
        "    // Conceptual x is [ 11.0, 12.0 ]\n",
        "    // Stored in h_x with size 3 and incx = 2: uses h_x[0] and h_x[0 + 1*incx] = h_x[2]\n",
        "    // h_x = [ 11.0, 99.0, 12.0 ] (99.0 is padding/unused)\n",
        "    h_x[0] = 11.0f;\n",
        "    h_x[1] = 99.0f; // This element will be skipped due to incx=2\n",
        "    h_x[2] = 12.0f;\n",
        "\n",
        "    // Initialize host y (optional, as beta=0)\n",
        "    for (int i = 0; i < M; ++i) {\n",
        "        h_y[i] = 0.0f;\n",
        "    }\n",
        "\n",
        "    // --- CUDA Initialization and Memory Allocation ---\n",
        "    cublasHandle_t handle;\n",
        "    float *d_A = NULL, *d_x = NULL, *d_y = NULL;\n",
        "\n",
        "    printf(\"Creating cuBLAS handle...\\n\");\n",
        "    CHECK_CUBLAS(cublasCreate(&handle));\n",
        "\n",
        "    printf(\"Allocating device memory...\\n\");\n",
        "    CHECK_CUDA(cudaMalloc((void**)&d_A, A_alloc_size * sizeof(float)));\n",
        "    CHECK_CUDA(cudaMalloc((void**)&d_x, X_alloc_size * sizeof(float)));\n",
        "    CHECK_CUDA(cudaMalloc((void**)&d_y, Y_alloc_size * sizeof(float)));\n",
        "\n",
        "    // --- Copy Data Host to Device ---\n",
        "    printf(\"Copying data from host to device...\\n\");\n",
        "    CHECK_CUDA(cudaMemcpy(d_A, h_A, A_alloc_size * sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CHECK_CUDA(cudaMemcpy(d_x, h_x, X_alloc_size * sizeof(float), cudaMemcpyHostToDevice));\n",
        "    // No need to copy h_y if beta is 0, but do it if beta != 0 or for consistency\n",
        "    CHECK_CUDA(cudaMemcpy(d_y, h_y, Y_alloc_size * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // --- Execute cuBLAS Sgemv ---\n",
        "    printf(\"Executing cublasSgemv...\\n\");\n",
        "    const float alpha = 1.0f;\n",
        "    const float beta = 0.0f;\n",
        "\n",
        "    // cublasStatus_t cublasSgemv(cublasHandle_t handle,\n",
        "    //                            cublasOperation_t trans,\n",
        "    //                            int m, int n,\n",
        "    //                            const float *alpha,\n",
        "    //                            const float *A, int lda,\n",
        "    //                            const float *x, int incx,\n",
        "    //                            const float *beta,\n",
        "    //                            float *y, int incy);\n",
        "    CHECK_CUBLAS(cublasSgemv(handle,\n",
        "                             CUBLAS_OP_N,    // No transpose of A\n",
        "                             M,              // Rows of A (3)\n",
        "                             N,              // Cols of A (2)\n",
        "                             &alpha,         // Scalar alpha (1.0)\n",
        "                             d_A,            // Pointer to device matrix A\n",
        "                             lda,            // Leading dimension of A (5)\n",
        "                             d_x,            // Pointer to device vector x\n",
        "                             incx,           // Increment for x (2)\n",
        "                             &beta,          // Scalar beta (0.0)\n",
        "                             d_y,            // Pointer to device vector y\n",
        "                             incy            // Increment for y (1)\n",
        "                             ));\n",
        "\n",
        "    // --- Copy Result Device to Host ---\n",
        "    printf(\"Copying result from device to host...\\n\");\n",
        "    CHECK_CUDA(cudaMemcpy(h_y, d_y, Y_alloc_size * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    // --- Cleanup ---\n",
        "    printf(\"Cleaning up...\\n\");\n",
        "    CHECK_CUDA(cudaFree(d_A));\n",
        "    CHECK_CUDA(cudaFree(d_x));\n",
        "    CHECK_CUDA(cudaFree(d_y));\n",
        "    CHECK_CUBLAS(cublasDestroy(handle));\n",
        "\n",
        "    // --- Print Result ---\n",
        "    printf(\"\\n--- Input --- \\n\");\n",
        "    printf(\"Matrix A (3x2 submatrix, column major, lda=%d):\\n\", lda);\n",
        "    printf(\"[ %5.1f %5.1f ]\\n\", h_A[0], h_A[lda*1 + 0]); // A[0,0], A[0,1]\n",
        "    printf(\"[ %5.1f %5.1f ]\\n\", h_A[1], h_A[lda*1 + 1]); // A[1,0], A[1,1]\n",
        "    printf(\"[ %5.1f %5.1f ]\\n\", h_A[2], h_A[lda*1 + 2]); // A[2,0], A[2,1]\n",
        "    // printf(\"Full A storage (Column Major):\\n\");\n",
        "    // for(int j=0; j<N; ++j) {\n",
        "    //     for(int i=0; i<lda; ++i) {\n",
        "    //         printf(\"%5.1f \", h_A[j*lda + i]);\n",
        "    //     }\n",
        "    //     printf(\"\\n\");\n",
        "    // }\n",
        "\n",
        "\n",
        "    printf(\"\\nVector x (Effective size %d, stored size %d, incx=%d):\\n\", N, x_alloc_count, incx);\n",
        "    printf(\"[ %5.1f ] (from h_x[0])\\n\", h_x[0]);\n",
        "    printf(\"[ %5.1f ] (from h_x[2])\\n\", h_x[2]);\n",
        "    // printf(\"Full x storage: [ \");\n",
        "    // for(int i=0; i<x_alloc_count; ++i) printf(\"%5.1f \", h_x[i]);\n",
        "    // printf(\"]\\n\");\n",
        "\n",
        "\n",
        "    printf(\"\\n--- Output --- \\n\");\n",
        "    printf(\"Result vector y (size %d, incy=%d):\\n\", M, incy);\n",
        "    printf(\"[ \");\n",
        "    for (int i = 0; i < M; ++i) {\n",
        "        printf(\"%8.2f \", h_y[i]);\n",
        "    }\n",
        "    printf(\"]\\n\");\n",
        "\n",
        "    // Expected result:\n",
        "    // y[0] = 1.0*11.0 + 4.0*12.0 = 11 + 48 = 59\n",
        "    // y[1] = 2.0*11.0 + 5.0*12.0 = 22 + 60 = 82\n",
        "    // y[2] = 3.0*11.0 + 6.0*12.0 = 33 + 72 = 105\n",
        "    printf(\"\\nExpected result:\\n[   59.00    82.00   105.00 ]\\n\");\n",
        "\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_A);\n",
        "    free(h_x);\n",
        "    free(h_y);\n",
        "\n",
        "    printf(\"\\nDone.\\n\");\n",
        "    return EXIT_SUCCESS;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 main.cu -o main.ex -lcublas"
      ],
      "metadata": {
        "id": "LNBPzLbZoFjb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./main.ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjhoiGD_oYSW",
        "outputId": "ad2f4fea-3d06-4a08-c31e-1b00f13a6fc4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing host data...\n",
            "Creating cuBLAS handle...\n",
            "Allocating device memory...\n",
            "Copying data from host to device...\n",
            "Executing cublasSgemv...\n",
            "Copying result from device to host...\n",
            "Cleaning up...\n",
            "\n",
            "--- Input --- \n",
            "Matrix A (3x2 submatrix, column major, lda=5):\n",
            "[   1.0   4.0 ]\n",
            "[   2.0   5.0 ]\n",
            "[   3.0   6.0 ]\n",
            "\n",
            "Vector x (Effective size 2, stored size 3, incx=2):\n",
            "[  11.0 ] (from h_x[0])\n",
            "[  12.0 ] (from h_x[2])\n",
            "\n",
            "--- Output --- \n",
            "Result vector y (size 3, incy=1):\n",
            "[    59.00    82.00   105.00 ]\n",
            "\n",
            "Expected result:\n",
            "[   59.00    82.00   105.00 ]\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}