{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <iostream>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#ifndef CUDA_CHKERR\n",
        "#define CUDA_CHKERR(err)                                          \\\n",
        "  do {                                                            \\\n",
        "    cudaError_t cuda_error = (err);                               \\\n",
        "    if (cuda_error != cudaSuccess) {                              \\\n",
        "      fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__,      \\\n",
        "              __LINE__, cudaGetErrorString(cuda_error));          \\\n",
        "      exit(EXIT_FAILURE);                                         \\\n",
        "    }                                                             \\\n",
        "  } while (0)\n",
        "#endif\n",
        "\n",
        "__global__ void helloFromGPU()\n",
        "{\n",
        "  printf(\"Hello World-Thread: %d\\n\", threadIdx.x);\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  helloFromGPU<<<1, 16>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "  CUDA_CHKERR(cudaGetLastError());\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPfT1RCoZ0uX",
        "outputId": "8dfd8617-89cc-4f3e-d5c0-2d3ccc1a01dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 main.cu -o main.ex"
      ],
      "metadata": {
        "id": "aXFsEkTFZ6E2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./main.ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5qV6trqZ_f-",
        "outputId": "5e529d91-1282-448d-de5d-295700b83f65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World-Thread: 0\n",
            "Hello World-Thread: 1\n",
            "Hello World-Thread: 2\n",
            "Hello World-Thread: 3\n",
            "Hello World-Thread: 4\n",
            "Hello World-Thread: 5\n",
            "Hello World-Thread: 6\n",
            "Hello World-Thread: 7\n",
            "Hello World-Thread: 8\n",
            "Hello World-Thread: 9\n",
            "Hello World-Thread: 10\n",
            "Hello World-Thread: 11\n",
            "Hello World-Thread: 12\n",
            "Hello World-Thread: 13\n",
            "Hello World-Thread: 14\n",
            "Hello World-Thread: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <iostream>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#ifndef CUDA_CHKERR\n",
        "#define CUDA_CHKERR(err)                                          \\\n",
        "  do {                                                            \\\n",
        "    cudaError_t cuda_error = (err);                               \\\n",
        "    if (cuda_error != cudaSuccess) {                              \\\n",
        "      fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__,      \\\n",
        "              __LINE__, cudaGetErrorString(cuda_error));          \\\n",
        "      exit(EXIT_FAILURE);                                         \\\n",
        "    }                                                             \\\n",
        "  } while (0)\n",
        "#endif\n",
        "\n",
        "__device__ double atomicAdd_CAS(double *address, double val)\n",
        "{\n",
        "    unsigned long long int* address_as_ull = (unsigned long long int*)address;\n",
        "    unsigned long long int old = *address_as_ull, assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        double old_double = __longlong_as_double(old);\n",
        "        double new_double = old_double + val;\n",
        "        unsigned long long int new_ull = __double_as_longlong(new_double);\n",
        "        old = atomicCAS(address_as_ull, assumed, new_ull);\n",
        "    } while (assumed != old);\n",
        "    return __longlong_as_double(assumed);\n",
        "}\n",
        "\n",
        "__global__ void initializeArray(double *arr, int N) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N) {\n",
        "        arr[i] = (double)i;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void atomicAddDemo(double *arr, int N) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N) {\n",
        "        atomicAdd_CAS(&arr[0], arr[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    int N = 1024;\n",
        "    double *arr_h, *arr_d;\n",
        "    size_t size = N * sizeof(double);\n",
        "\n",
        "    arr_h = (double*)malloc(size);\n",
        "    CUDA_CHKERR(cudaMalloc(&arr_d, size));\n",
        "\n",
        "    initializeArray<<<(N + 255) / 256, 256>>>(arr_d, N);\n",
        "    CUDA_CHKERR(cudaGetLastError());\n",
        "    CUDA_CHKERR(cudaDeviceSynchronize());\n",
        "\n",
        "\n",
        "    atomicAddDemo<<<(N + 255) / 256, 256>>>(arr_d, N);\n",
        "    CUDA_CHKERR(cudaGetLastError());\n",
        "    CUDA_CHKERR(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHKERR(cudaMemcpy(arr_h, arr_d, size, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    printf(\"Sum: %f\\n\", arr_h[0]);\n",
        "\n",
        "    free(arr_h);\n",
        "    CUDA_CHKERR(cudaFree(arr_d));\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAoO9r9nbk1h",
        "outputId": "4d5b12a9-8494-47fb-df46-cfb02184bbd8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 main.cu -o main.ex"
      ],
      "metadata": {
        "id": "iWr7nnsAbvM4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./main.ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiTHTAyibwG8",
        "outputId": "c1c822ca-e283-4c04-fb0a-c7bfc20bc140"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum: 523776.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// I did not include error check. recommend to add!\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "template <typename T>\n",
        "__global__ void reduceSumV1(T *g_idata, T *g_odata, unsigned int n) {\n",
        "    __shared__ T sdata[2*BLOCK_SIZE];\n",
        "\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int start = 2 * blockIdx.x * blockDim.x;\n",
        "\n",
        "    sdata[tid] = (start + tid < n) ? g_idata[start + tid] : 0;\n",
        "    sdata[tid+BLOCK_SIZE] = (start + tid + BLOCK_SIZE < n) ? g_idata[start + tid + BLOCK_SIZE] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int s = 1 ; s <= blockDim.x ; s *= 2) {\n",
        "        if (tid % s == 0) {\n",
        "            sdata[2*tid] += sdata[2*tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        g_odata[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "__global__ void reduceSumV2(T *g_idata, T *g_odata, unsigned int n) {\n",
        "    __shared__ T sdata[2*BLOCK_SIZE];\n",
        "\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int start = 2 * blockIdx.x * blockDim.x;\n",
        "\n",
        "    sdata[tid] = (start + tid < n) ? g_idata[start + tid] : 0;\n",
        "    sdata[tid+BLOCK_SIZE] = (start + tid + BLOCK_SIZE < n) ? g_idata[start + tid + BLOCK_SIZE] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int s = blockDim.x; s > 0; s /= 2) {\n",
        "        if (tid < s) {\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        g_odata[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "T cpuReduceSum(const std::vector<T>& data) {\n",
        "    T sum = 0;\n",
        "    for (const auto& val : data) {\n",
        "        sum += val;\n",
        "    }\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    unsigned int n = 1000000;\n",
        "    std::vector<float> h_data(n);\n",
        "\n",
        "    for (unsigned int i = 0; i < n; ++i) {\n",
        "        h_data[i] = static_cast<float>(rand() % 10);\n",
        "    }\n",
        "\n",
        "    float *d_idata, *d_odata;\n",
        "    cudaMalloc((void **)&d_idata, n * sizeof(float));\n",
        "\n",
        "    unsigned int numBlocks = (n + 2 * BLOCK_SIZE - 1) / (2*BLOCK_SIZE);\n",
        "    cudaMalloc((void **)&d_odata, numBlocks * sizeof(float));\n",
        "\n",
        "    float *d_idata_v2, *d_odata_v2;\n",
        "    cudaMalloc((void **)&d_idata_v2, n * sizeof(float));\n",
        "    cudaMalloc((void **)&d_odata_v2, numBlocks * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_idata, h_data.data(), n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_idata_v2, h_data.data(), n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    reduceSumV1<float><<<numBlocks, BLOCK_SIZE>>>(d_idata, d_odata, n);\n",
        "    reduceSumV2<float><<<numBlocks, BLOCK_SIZE>>>(d_idata_v2, d_odata_v2, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    std::vector<float> partialSumsV1(numBlocks);\n",
        "    cudaMemcpy(partialSumsV1.data(), d_odata, numBlocks * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::vector<float> partialSumsV2(numBlocks);\n",
        "    cudaMemcpy(partialSumsV2.data(), d_odata_v2, numBlocks * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float finalSumV1 = cpuReduceSum(partialSumsV1);\n",
        "    float finalSumV2 = cpuReduceSum(partialSumsV2);\n",
        "\n",
        "    float cpuSum = cpuReduceSum(h_data);\n",
        "    std::cout << \"CUDA Reduction V1 Sum: \" << finalSumV1 << std::endl;\n",
        "    std::cout << \"CUDA Reduction V2 Sum: \" << finalSumV2 << std::endl;\n",
        "    std::cout << \"CPU Sum: \" << cpuSum << std::endl;\n",
        "    if (std::abs(finalSumV1 - cpuSum) < 1e-5 && std::abs(finalSumV2 - cpuSum) < 1e-5 ) {\n",
        "        std::cout << \"Results match!\" << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"Error: Results do not match!\" << std::endl;\n",
        "    }\n",
        "\n",
        "    cudaFree(d_idata);\n",
        "    cudaFree(d_odata);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-WCAHAhhn7_",
        "outputId": "c6af5793-78c4-4b78-83b6-f4f162117171"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 main.cu -o main.ex"
      ],
      "metadata": {
        "id": "pVZn2uduh_6E"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./main.ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj2hrZ17iAw3",
        "outputId": "5cb12075-166e-40e4-a86d-c517c51b17ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Reduction V1 Sum: 4.50273e+06\n",
            "CUDA Reduction V2 Sum: 4.50273e+06\n",
            "CPU Sum: 4.50273e+06\n",
            "Results match!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#include <algorithm>\n",
        "\n",
        "// I did not include error check. recommend to add!\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Note: this code is for illustrative purpose. Deos not work for more than one block.\n",
        "template <typename T>\n",
        "__global__ void prefixSumKernelMultiple(T *g_idata, T *g_odata, int n) {\n",
        "    __shared__ T sdata[2 * BLOCK_SIZE];\n",
        "\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * (BLOCK_SIZE * 2) + tid;\n",
        "\n",
        "    T val0 = (i < n) ? g_idata[i] : 0;\n",
        "    T val1 = (i + BLOCK_SIZE < n) ? g_idata[i + BLOCK_SIZE] : 0;\n",
        "\n",
        "    sdata[tid] = val0;\n",
        "    sdata[tid + BLOCK_SIZE] = val1;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int stride = 1; stride < 2 * BLOCK_SIZE; stride *= 2) {\n",
        "        int index = (tid + 1) * stride * 2 - 1;\n",
        "        if (index < 2 * BLOCK_SIZE) {\n",
        "            sdata[index] += sdata[index - stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "     for (int stride = BLOCK_SIZE; stride > 0; stride /= 2) {\n",
        "        int index = (tid + 1) * stride * 2 - 1;\n",
        "        if (index + stride < 2 * BLOCK_SIZE) {\n",
        "            sdata[index + stride] += sdata[index];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (i < n) {\n",
        "        g_odata[i] = sdata[tid];\n",
        "    }\n",
        "    if (i + BLOCK_SIZE < n) {\n",
        "        g_odata[i + BLOCK_SIZE] = sdata[tid + BLOCK_SIZE];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "void cpuPrefixSum(std::vector<T>& data) {\n",
        "    for (size_t i = 1; i < data.size(); ++i) {\n",
        "        data[i] += data[i - 1];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 2*BLOCK_SIZE; // only one block for demo!\n",
        "    std::vector<int> h_data(n);\n",
        "\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        h_data[i] = i + 1;\n",
        "    }\n",
        "\n",
        "    std::vector<int> h_cpuResult = h_data;\n",
        "    cpuPrefixSum(h_cpuResult);\n",
        "\n",
        "    int *d_idata, *d_odata;\n",
        "    cudaMalloc((void **)&d_idata, n * sizeof(int));\n",
        "    cudaMalloc((void **)&d_odata, n * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_idata, h_data.data(), n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int numBlocks = (n + (BLOCK_SIZE * 2) - 1) / (BLOCK_SIZE * 2);\n",
        "\n",
        "    prefixSumKernelMultiple<int><<<numBlocks, BLOCK_SIZE>>>(d_idata, d_odata, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    std::vector<int> h_gpuResult(n);\n",
        "    cudaMemcpy(h_gpuResult.data(), d_odata, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    bool success = true;\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        if (h_gpuResult[i] != h_cpuResult[i]) {\n",
        "             std::cout << \"Error at index \" << i << \": GPU = \" << h_gpuResult[i]\n",
        "                      << \", CPU = \" << h_cpuResult[i] << std::endl;\n",
        "            success = false;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if(success)\n",
        "    {\n",
        "        std::cout << \"Prefix Sum calculation successful!\" << std::endl;\n",
        "    }\n",
        "\n",
        "    cudaFree(d_idata);\n",
        "    cudaFree(d_odata);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahb_hspvjJjV",
        "outputId": "73cdf780-6f7c-4d2c-bfdf-2f1386c7bc61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 main.cu -o main.ex"
      ],
      "metadata": {
        "id": "Bej0JdVjx9Bl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./main.ex"
      ],
      "metadata": {
        "id": "prZsv7CQyKNC",
        "outputId": "3a0fcc10-1340-48af-9e23-7b98c46582cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prefix Sum calculation successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// I did not include error check. recommend to add!\n",
        "\n",
        "__global__ void countCharactersKernel(const char* text, int* counts, int textSize) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (tid < textSize) {\n",
        "        char c = text[tid];\n",
        "\n",
        "        if (c >= 'A' && c <= 'Z') {\n",
        "            c += 32; // Convert to lowercase\n",
        "        }\n",
        "\n",
        "        if (c >= 'a' && c <= 'z') {\n",
        "            atomicAdd(&counts[c - 'a'], 1); // Increment the count for the corresponding letter\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    std::string text = \"High Performance Computing: Tools and Applications\";\n",
        "    int textSize = text.length();\n",
        "\n",
        "    std::vector<int> h_counts(26, 0); // 26 letters (a-z)\n",
        "\n",
        "    char* d_text;\n",
        "    int* d_counts;\n",
        "    cudaMalloc(&d_text, textSize * sizeof(char));\n",
        "    cudaMalloc(&d_counts, 26 * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_text, text.c_str(), textSize * sizeof(char), cudaMemcpyHostToDevice);\n",
        "    cudaMemset(d_counts, 0, 26 * sizeof(int));\n",
        "\n",
        "    int blockSize = 256;\n",
        "    int numBlocks = (textSize + blockSize - 1) / blockSize;\n",
        "\n",
        "    countCharactersKernel<<<numBlocks, blockSize>>>(d_text, d_counts, textSize);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(h_counts.data(), d_counts, 26 * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::cout << \"Character Counts:\" << std::endl;\n",
        "    for (int i = 0; i < 26; i++) {\n",
        "        if (h_counts[i] > 0) { // Only print letters that actually appear\n",
        "            std::cout << char('a' + i) << \": \" << h_counts[i] << std::endl;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaFree(d_text);\n",
        "    cudaFree(d_counts);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csZspVv1jFLO",
        "outputId": "e8b0d075-c34b-4bb3-9231-574b3c315875"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 main.cu -o main.ex"
      ],
      "metadata": {
        "id": "AvrNcINQHrvV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./main.ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH35RivuHvDe",
        "outputId": "d4ad992e-92f6-458a-fd4a-ee7fe636160d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character Counts:\n",
            "a: 4\n",
            "c: 3\n",
            "d: 1\n",
            "e: 2\n",
            "f: 1\n",
            "g: 2\n",
            "h: 2\n",
            "i: 4\n",
            "l: 2\n",
            "m: 2\n",
            "n: 4\n",
            "o: 5\n",
            "p: 4\n",
            "r: 2\n",
            "s: 2\n",
            "t: 3\n",
            "u: 1\n"
          ]
        }
      ]
    }
  ]
}